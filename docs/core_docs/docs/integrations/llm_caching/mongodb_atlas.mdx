# MongoDB Atlas Semantic Cache

This page documents the MongoDB Atlas integration for **semantic caching** of LLM generations.

Semantic caching allows you to cache and retrieve generations based on vector similarity, so similar prompts can share cached results.

## Install dependencies

You'll first need to install the [`@langchain/mongodb`](https://www.npmjs.com/package/@langchain/mongodb) as well as [`mongodb`](https://www.npmjs.com/package/mongodb):

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install mongodb @langchain/mongodb @langchain/core
```

## Set up the cache collection

You will need the `mongodb` driver package to manage your db, collection(s), and (vector search) indexes. The `@langchain/mongodb` package has the integration for LangChain and expects a ready-to-use collection and vector search index.

You can set up a vector collection either through the MongoDB Atlas UI or with commands such as the following:

```
import { MongoClient } from 'mongodb';

const client = new MongoClient(process.env.MONGODB_ATLAS_URI);
await client.connect();
const db = client.db('db_name');
const collection = db.collection('collection_name');

// create a search index. The dimensions need to match your embedding dimensions.
await collection.createSearchIndex({
    name: 'default',
    definition: {
    mappings: {
        dynamic: true,
        fields: {
        embedding: { dimensions: 1024, similarity: 'cosine', type: 'knnVector' },
        },
    },
    },
});
```

Note that the initial creation of a Vector Search index takes some time (it may take more than 30 seconds). If you query the vector index while it is initializing, you may get an error or an empty response. Also, each time that a new document (vector embedding, etc.) is added, the index needs to update before it can return the new document as part of its response. You can query the vector index while it is being updated, but it will return data based on the old index.

## LLM call with semantic caching

Assuming that you have set up your MongoDB Atlas collection and Vector Search Index, you can set up semantic caching with

```
import { MongoDBAtlasSemanticCache } from '@langchain/mongodb';

// [other code...]

// Get your LLM cache collection
const cache llmSemCacheCollection = db.collection(LLM_SEMANTIC_CACHE_COLLECTION);

// optionally, in case you are also caching embeddings, get those collections as well
const documentEmbeddingsCache = new MongoDBStore({ collection: db.collection(DOC_EMBEDDINGS_CACHE) });
const queryEmbeddingsCache = new MongoDBStore({ collection: db.collection(QUERY_EMBEDDINGS_CACHE) });

// Set up embedding model
// In this case using HuggingFace and with cacheBackedEmbeddings to cache the embeddings
const cacheBackedEmbeddings = CacheBackedEmbeddings.fromBytesStore(
    new HuggingFaceInferenceEmbeddings({
    apiKey: process.env.HUGGINGFACEHUB_API_KEY,
    model: process.env.HUGGINGFACEHUB_MODEL,
    provider: process.env.HUGGINGFACEHUB_PROVIDER,
    }),
    documentEmbeddingsCache,
    {
    namespace: process.env.HUGGINGFACE_EMBEDDING_MODEL,
    queryEmbeddingStore: queryEmbeddingsCache,
    },
);

// Set up Semantic Cache
const llmSemanticCache = new MongoDBAtlasSemanticCache(
    llmSemCacheCollection,
    cacheBackedEmbeddings, // Embedding model should be passed separately
    { scoreThreshold: 0.99 }, // Optional similarity threshold settings
);

// Set up LLM
const llm = new ChatTogetherAI({
    apiKey: process.env.TOGETHERAI_API_KEY,
    model: process.env.TOGETHERAI_MODEL,
    cache: llmSemanticCache,
});

// [other code; set prompts to be your LLM prompt]

// Call generate to get the response. The response will be fetched from
// the cache if there is a hit with similarity that meets the scoreThreshold
// criteria. Otherwise, the LLM will be called
const results = await llm.generate(prompts);
```
